{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "MTNIsRZv_f8D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dsNDWRHznMzI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#process : images(input_dim) -> hidden dim -> mean and std deviation vectors -> reparametrization -> latent dimension -> output_dim\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "  def __init__(self,input_dim, h_dim = 200,z_dim = 20): #h_dim:hidden_dimension\n",
        "    super().__init__()\n",
        "\n",
        "    #encoder\n",
        "    self.img_2hid = nn.Linear(input_dim,h_dim) #taking input image into a hidden dimension\n",
        "    self.hid_2mu = nn.Linear(h_dim,z_dim) #z_dim is dimension of mu vector or std deviation vector or the latent vector (all 3 have the same dim)\n",
        "    self.hid_2sigma = nn.Linear(h_dim,z_dim) #for sd vector\n",
        "\n",
        "    #reparamterization will be taken care of later(in the forward class).for now,assuming reparamaterization is done,we start from the latent vector in the decoder\n",
        "\n",
        "    #decoder\n",
        "    self.z_2hid = nn.Linear(z_dim,h_dim)\n",
        "    self.hid_2img = nn.Linear(h_dim,input_dim)\n",
        "\n",
        "\n",
        "\n",
        "  def encode(self,x): #q_phi(z|x)\n",
        "    h = F.relu(self.img_2hid(x)) #mapping input into hidden dim followed by relu activation\n",
        "    mu,sigma = self.hid_2mu(h),self.hid_2sigma(h) #?) why not ReLu here\n",
        "    return mu,sigma\n",
        "  \n",
        "  def decode(self,z): #p_theta(x|z) again assuming we already have the latent vector z\n",
        "    h = F.relu(self.z_2hid(z))\n",
        "    img = self.hid_2img(h)\n",
        "    img = torch.sigmoid(img) #this step is specifically for our use-case,that is MNIST dataset,which assumes the pixel values are in b/w 0 and 1\n",
        "    return img\n",
        "\n",
        "  def forward(self,x,z):\n",
        "    #encode\n",
        "    mu,sigma = self.encode(x)\n",
        "    #now we apply reparamterization\n",
        "    epsilon = torch.rand_like(sigma)\n",
        "    z_reparametrized = mu+sigma*epsilon\n",
        "    #decode\n",
        "    x_reconstructed = self.decode(z_reparametrized)\n",
        "    return x_reconstructed,mu,sigma #remember that we need this mu and sigma to compute the loss functions (especially while computing the KL divergence)"
      ],
      "metadata": {
        "id": "AProO9V9oKmg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  x = torch.randn(4,28*28) #batch_size = 4 and 28*28 images\n",
        "  vae = VariationalAutoEncoder(input_dim = 784) #input dim is 28*28 flattened as an input vector. The other two paramters are already initialised above"
      ],
      "metadata": {
        "id": "_IxHZY6svssY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "9hDqrTkz_i_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm #for progress bar\n",
        "import torchvision.datasets as datasets  #to get MNIST\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "E2mhsBwP_TyC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing hyperparameters\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INPUT_DIM = 784\n",
        "H_DIM = 200\n",
        "Z_DIM = 20\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "LR_RATE = 3e-4"
      ],
      "metadata": {
        "id": "a7sLg-As_n-I"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading dataset\n",
        "dataset = datasets.MNIST(root = \"dataset/\",train = True,transform = transforms.ToTensor(),download = True) #.ToTensor also divides by 255(scaling)\n",
        "train_loader = DataLoader(dataset = dataset,batch_size = BATCH_SIZE,shuffle = True)\n",
        "model = VariationalAutoEncoder(INPUT_DIM,H_DIM,Z_DIM).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(),lr = LR_RATE)\n",
        "loss_fn = nn.BCELoss(reduction = 'sum') #might use MSE also"
      ],
      "metadata": {
        "id": "fHZAAJo2CQE8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loop = tqdm(enumerate(train_loader))\n",
        "  for i,(x,_) in loop:\n",
        "    #Forward pass\n",
        "    x = x.to(DEVICE).view(x.shape[0],INPUT_DIM)\n",
        "    x_reconstructed,mu,sigma = model(x,Z_DIM)\n",
        "\n",
        "    #Compute loss\n",
        "    reconstruction_loss = loss_fn(x_reconstructed, x)\n",
        "    kl_div = -torch.sum(1+torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2)) #-ve sign was not part of the paper but pytorch minimizes loss by default\n",
        "\n",
        "    #Backpropagation\n",
        "    loss = reconstruction_loss + kl_div\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loop.set_postfix(loss = loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w12aCTIKD7eP",
        "outputId": "3ff37f28-2ac1-41b6-9240-032aa0433f6c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1875it [00:32, 57.81it/s, loss=4.3e+3]\n",
            "1875it [00:31, 59.48it/s, loss=3.59e+3]\n",
            "1875it [00:32, 57.04it/s, loss=3.73e+3]\n",
            "1875it [00:32, 58.13it/s, loss=3.24e+3]\n",
            "1875it [00:32, 58.02it/s, loss=3.51e+3]\n",
            "1875it [00:32, 57.67it/s, loss=3.49e+3]\n",
            "1875it [00:31, 58.89it/s, loss=3.43e+3]\n",
            "1875it [00:32, 57.44it/s, loss=3.16e+3]\n",
            "1875it [00:31, 58.97it/s, loss=3.33e+3]\n",
            "1875it [00:32, 57.08it/s, loss=3.47e+3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(digit, num_examples=1): #digits and number of examples we want to generate\n",
        "\n",
        "#Generates (num_examples) of a particular digit.Specifically we extract an example of each digit,then after we have the mu, sigma representation foreach digit we can sample from that.\n",
        "\n",
        "#After we sample we can run the decoder part of the VAE and generate examples.\n",
        "    images = []\n",
        "    idx = 0 #getting 10 images\n",
        "    for x, y in dataset:\n",
        "        if y == idx:\n",
        "            images.append(x)\n",
        "            idx += 1\n",
        "        if idx == 10:\n",
        "            break\n",
        "\n",
        "    encodings_digit = [] #getting mu and sigma for each image\n",
        "    for d in range(10):\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = model.encode(images[d].view(1, 784))\n",
        "        encodings_digit.append((mu, sigma))\n",
        "\n",
        "    mu, sigma = encodings_digit[digit] #generate new images after reparametrization\n",
        "    for example in range(num_examples):\n",
        "        epsilon = torch.randn_like(sigma)\n",
        "        z = mu + sigma * epsilon\n",
        "        out = model.decode(z)\n",
        "        out = out.view(-1, 1, 28, 28)\n",
        "        save_image(out, f\"generated_{digit}_ex{example}.png\")\n",
        "\n",
        "for idx in range(10):\n",
        "  inference(idx,num_examples = 1)"
      ],
      "metadata": {
        "id": "xc1ZGFBFJpmE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        " \n",
        "for i in range(10):\n",
        "  img = Image.open('/content/generated_'+str(i)+'_ex0.png')\n",
        "  img.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "-seI4_UQWsnF",
        "outputId": "ebeed8c4-378f-4295-9054-1323dba7675f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133B7C0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADYUlEQVR4nO2UzUsqURjG58zMmZipcdKwDIWgScI+lEDoA0QXFf0BRhDkpkXL/oq20aJ9BLVoFYYMBUES0caCUIKCEhFTydLMdNQzM+cuBuJm3j7uvZsL99mel995nvc8HIL4r78t8NU5AGia5jgOY4wQQgipqoox/k0oAICiqI6OjomJCUEQZFmOx+PZbFZV1Xw+X6vV3qPpT6EkSbIsazQaaZq2WCxms3lycrK3tzeXy0Uike3t7fv7e03TvuFUT22xWAwGgyAIHMdZrdZAIDAyMsKyrCzLt7e3CwsL19fXP/slP7Vps9k8Ho8oigzD0DTtcDj6+vo4jmMYRhAEp9O5vr5uMBi+Gh8A4HK5NjY2zGZzMpnc29uDEPp8PoxxvV6nKIokSZqm3W63x+ORJOl1Cb+EAgCGhobC4XBrayvG+Pn5uV6vPzw8SJIUi8V4nl9eXh4eHqYoimXZqampg4ODhs02IYqieHd3p2mapmmlUml+ft5kMnEcByGkKIphmLm5uUKhoCgKQmh/f7+lpeXjTRJdXV2JREJRFL03Y2NjJPlm+wCA/v7+aDSqd/by8pLn+Y922t7eHgwGbTYbACAej09PTycSifdlLBaLJEkCAAiCwBhDCH/pEUK4tbWlqqqmadVqdWlpqek0AMBut6fTaVVVEULhcPhnp42Vstvtfr9fD5tKpSRJUhTlPRFCaLVaIYSappXL5bOzs1qt1jw+AMDv90MI9dKsrq5mMpmG4PoPQJKkpmmxWMxisYRCoc3NTYRQ8+w0TQeDQUVR6vX67u4uy7INV7IsOz4+3tPTw/O8yWRyOp0+n89sNjc84xunJEl2dnYihHK53MrKSrVafcUxDOP1el0uVzKZvLq6qlQqGONisQgA0GvX3CZBEAzDhEKhx8fHo6MjQRBeiaIo7uzsZDKZ09NTh8NB0598Q2+OMcapVAohVCqVWJZ9eXkhCEIUxcPDw+7uboTQ+fm53t+PoW8EIVxcXMxms+l0em1tbXR0dHZ2Vm+4oig3Nzd6eb9BJAiCoqiZmZmLi4tyufz09JTL5fL5vCzLCKFMJjMwMPBtoq62trZAIBCNRguFgizLlUqlVCpFIpHBwcGvE5vMURRlNBrdbrfX60UIHR8fn5ycvDbhT/WbYf8l/QBjuaSy81uw+gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133B820>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADF0lEQVR4nO1VT0vrThSd/5nQJFqKIrgoqKAWKrh14SdwIbjwAwi679bvI34A17UuBVcKooUiKmmhQhNM0ybtTPIW15fXV5s+/K1/d1GGzJxzz9x7ewah/+M/BcYYYzx766dchBDLsqSUvu9rrdM0TdMUIQS/X2dm5s+TQCldXV1dX19PkgRYMMaU0ikIyU5jjAkhhBDGGCEkOwQAQoiUslKpnJycrK2tmaZJCAGBkGBS6R+BlFJKKWNMCMEYy9IwxhhjCwsLBwcHd3d3rVbr/Pzctm1IPPNObKpeCCEhhFJKKUUp5ZxjjIvF4vHxca1WW15eHo1Gvu8Ph8MkSfLq/kUKt0AIGYahtU6SBDRyzjnn1Wr16OhocXGREJKmaaPR0FrnMf6lFHijKAIkfFFKpWmKMe71ekqpJEmen587nc4cRjTVfeAFpbBQSo3H45eXl8fHx36/73levV6P43g+Kfv+KZsVWMRx7Lpuo9Eol8tKqaurq8ndmZE7/BkMWmzbdqVSCcOw2+0OBgOt9ZxezSMF1bAghJimyTmnlDqOwxh7f3+Pomim3hnXn+QFRoyxlLJYLAohdnd3t7a2bm5u2u12HjCXFGPMGDNN03GcnZ2dw8PDUqkURVG32/V9P4qi0Wj0Y9I0TbXWQoi9vb1arba9vR1F0e3t7fX1dbPZdF0X3GTORXPFGoaxv7/farXCMLy8vNzY2LAsy7KsQqHAGMvznXk1BZtYWVlhjD08PJyengZBgBAihBiGwRjL3GQKOG1935VWq9Vms3l2dhYEQfo7hBCO4xQKhUnry4T/o/tLS0ta64uLi6enp0wRTJVt2/1+37btJEmCIIjjOKvyX6SU0sl5FkIYhvH5+Xl/fz8ej7NM0MNyuby5udnpdN7e3l5fXz8+PrKsf0hBPJQfPEVKmSRJvV53XTcDwEJrXSqVwjDknDPGpv4F0y6FEOKcSyk554QQz/M8z+v1ehkGaielHA6H7XYbdsMwBD/7OvO9lPAECCHA2weDwSQAeGFXaw2WNjUAP35NIaA+ULE5T8DPIm/sIX4BWl7VJZcRf8IAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133AEF0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADx0lEQVR4nO2VS0vzWBjHz0matGnS1rZUoSkualHIQhFXigvFjbpw71p0J34Fv4W4FKniyr1dtQpKBRVsi5Vob1SpvWmT5tbknFmEKfM6Vd/ZDAzMszuc5/zyf64B4H/7Txj8Z94QYowhHPAKY/y7UAghSZJOp5NlWZ/Pp6pqr9fDGPt8PoqiDMNot9uapum6bllWH+34Bgch9Pl8giCsra0tLCxQFCXLMkVRoVCI4zhFUZ6fnxOJRCKReHl5abVavV7PfvslFADgdrsnJyd3dnZmZ2c5jgMAWJZFUZTT6QQAGIaBEIrFYrVajSAISZJM0/xBKUEQbrd7dHRUEASHw4Exxhjruo4QIggCYyzLsiiKxWKx3W7rum77fKfULoVhGB6Pp9lsKopiGEa5XL67u+N5fm5uzuv15nK509PTXC5nmqZlWZZl2WX8EmrrMgwjn8+nUilJktLpdLFYhBDOz89PTU11u92np6eHh4dSqaRpmp2QH5TaHqZplsvlVCpVr9ff3t4URRkbG1tfXx8fH2+1Wo1Go1qtdrtdAIBpmuDPhvuhUAihj4+PcrmMEHI6nYFAYHNzc2ZmxuFwVCqV29vbTqdjd5KmaZZl/awUQkgQhMvlCofDXq+Xoqjl5eWlpSWXy1Wv11OplCiKdrlpmqZpWpbl/tsBUBvncDg8Hg/P84uLi4IgxGKxSCRC07Sqqslk8vz83DCMQCCAMXa5XBBC0zQ1TRscvq3O4/GEw+FYLLa1tSUIgsvlYlmWJEnTNEVRPDo6ajQaNE0Hg0Ge5wmCAADk8/lut6vr+gAoy7IrKyuxWMztdvM8HwqFGIbhOI4kSYxxvV4/OTl5fHxst9vBYJBhGJ7nh4aGMMaKooiiOAAKIZyent7e3vb5fJVKRVVVlmUZhiFJEgBgGMbx8fHh4eH7+7s9CBjjkZGRSCQiy3J/nD5DCYJYXV2dmJhACOm6TlFUMBi0R0VV1f39/d3d3X7iLMuqVqtnZ2eSJEWj0VqtZjfWAOjw8DBN0wihcDiMMSZJstvtVqvVjY2NdDqNEOo722JN07y6uhJF8fX1tX/7OfxSqdRsNhmGQQghhDKZzMHBQTwet5v874YxliRJlmXLsgYrRQgVCoVCoRCNRjudzv39/d7e3vX1dX+nDYTaGxZ8NaYEQWQymWQyqeu6qqrxePzm5qb//a/Mjvqvm/8XqJ37bDbr9/svLi4uLy/tEn8P/UQEABCfriGEzWYzk8lks1lZln8k2vvs01/rlwNJkhzH+f1+hmFKpZKqqr8j81+yPwAzCWNtQskjUwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133A980>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADlklEQVR4nO2UzUtyTRjGnTmjR4+SZWJm36GhRmQRFBFYRFSEi5Ba1aJltPffKWgbRNSiiBYRFURkRPZBX3TUitRTph6P58wc38UBHx8fy2fxrl7eazszv7nua+65Var/9W8LVFgGAEKIEKqqqmpqampoaGhtbb24uLi5ueE4DmOcz+f/CgoAAAAghCwWi8fjmZqaGhoaamlpoWkaAEAI4Xn+6enp4OBgfX09GAzyPE8IKSagEiJFUQzDGI3Gtra2wcHB8fFxj8djMBgoilKpVIQQQRDS6bRer+/p6cEYi6J4fn4uy3Kx5d+gBY8YYwBAKpXCGEuSJIoihDCVSh0eHm5ubr69vVmt1vb2dowxhBBCWBJCmfKVfWq1urq62mw29/X1mUwmjPHe3h7LsrlcTonY7XZDCF9fXx8eHrLZbAVowbVyAUVRCCElTUmSIIRarbajo8PlcrEsy7JsNBqVJOmnTAtSKiKEyLJMCNFoNEob0DTtcDjGxsby+fzV1VU8HscYl5z9FlpMBwAYjUaTySQIglartdvtsizncrlYLJbNZv/sqspQAADDMAMDAzabLRqNfn5+CoLAcVw4HOY4TpblP49UgCrE6enphYWFWCy2s7PD8zyEkGGY5+dnQRDKNn95qPKRIIQWi2VxcXF2dtZgMAAAmpubnU6ny+XieX5jY6OszW+hNE1brVav1zs/P+/xeBBCiURCEASv12u3200mUywWkySprM3yUIRQY2PjxMTE5OSkzWZLpVL39/dHR0cURfl8PovFopSv0WgAAH9VPoSwtrbWarVeX1/f3t5GIpH39/dsNivLstvt9vv9EEKVShWPx9PpdFmbpVAIYU1NTXd3N0Lo9PT04+Oj0NUAgEQikUwmCSEY4+3tbZ7nv4PCEo8+n294eFiW5WQyWfxPAAB2u52m6a+vr1AotLy8LIpi5UwRQv39/UtLSwaD4fLysngTAKCurm5ubk6v10cikdXV1ZeXl++e/jfpdLpAIJBIJNLpdCgU6u3tRQhBCGmaHh0dPT4+5jguEomsra2NjIzodDol3J+cKkNPFEVCiFqtdjgcu7u7Jycn4XDY6XR2dnbqdDpJkliWXVlZOTs7U6byd6//S1qttqura2trS0mzMEkzmUwmk7m7uwsEAvX19RqNhqIoCKEyxsrq14IyQ81m88zMjN/vZxgGY/z4+BgMBvf394PBoCiKlUP8QT+4+G/pH37+xp5mVox1AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133BA30>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADVklEQVR4nO2Vu08qWxTGZ+95EMaRhwGVEISgYELjK1IYYuKrMjE+OmO0orWxtDJY+icQC/8DjZU2JlaaaGGCZMQBZoKKIgg4gzCzdZ9iTs5F5OrV9p5Vz/rtb397rW8I4n9d4GdtJElCCCmKAgBomvb6+ooxxhj/EAoAYFnWYrE4HA6GYfx+f61WS6VS8Xi8XC7rXOpbRAhhS0uLwWAgCIJhmNnZ2cHBQZfLlUgkIpHI2dkZQoggCNig4nNiR0eH1+s1GAzValWSJFmWe3p6HA5Hb2+v2+0mSfL3l/Vtf0xpemufzxeJRNxud6FQKBaLuVxub28vmUw+Pz8nEomTkxNVVb91byIQCGSzWUEQfD6ffiGSJDs7Ozc3N/f39xcWFoxG4/eINE2n02lVVQ8PD3VDAQA0TbtcrqWlpampKbPZXG/d1w8FAFhfX3c6nRjjaDSqaRoAgKKo7u5uo9GYTCZFUVQU5RPrmpTf7y8Wi5qmpdNpm80GIaRpOhgMrq2tLS4uejwehmEaXvgLpRzHbW1tQQjz+fz29vbLywvLskNDQxMTEzabjef5fD6PEGqQ2RwKACBJkmXZubk5s9ksiuLT01M8HrfZbCaTaXR01Ol0XlxcCILwkdgECiFkWdZqtXq93lAo1N/fX6vVLi8vHx8f7XZ7KBQaGxsLBAKCIPA8XywW9Za3t7d69DsoSZIej2d4eHh1dZVlWQghwzAIodvb20ql0tfX19XVFQwGq9Xq0dGRLl/TtAZiI7S1tXVmZiYcDlssFoyxoiiqqqZSKZ7nLRbL+Pi43W4HAPA8H41GJUnSc+Sje/9AAQDt7e0jIyOyLFcqlUKhcHp6enNzc319jRDa2NjQpyoWi62srGQymU9m6B2UZVlJkkRRvLu7Ozg4eHh4UBSFoqjp6emBgQGapq+urubn5zOZzL/hmkBrtVo6nY7FYslkMpvNIoQAAEajcXJykqbpcrm8vLz8JZGoDxSMsSzLuVwOIVQqlf74ZTKZrFbr/f19OBw+Pz//kvhOKca4VCrpqaNPH8ZYX5Xj4+OdnZ3d3d3/uIvv1oumaY7jVFVFCGmaRhAESZJtbW0cx4miqAfwt6EAAAihrloXpWcHxlg/4+cFAKgPiM9/B3/rY/0CaxawefidMqQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133B430>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADLklEQVR4nO2UvUvrUBjGzzk52qSpST8UqamtYoqoEBeHiKMgjoKLm/jvOPkPOLm4OvgBDiK2DopKISCtbiFNSasl2pqmSU7uEOittd7rvdfxPmM47y/Pe973OQD813cL/k0NhBRFsSxLCPE8z3Vd13UJIX8PRQhhjDmOGxsbGx8fRwi5rlsqlVRVbbfbwRncYwEA4Pv+ZwZDoRDP84ODgxhjjPHU1NTq6irDMIVCYXt7W9f1oBb3VP6CmEwmt7a2HMc5PDx8fn5uNBqEEEEQotEoxpjneV3X+zj9jAgA4Dgul8slEolcLndwcPD29saybCaTicfjNE13uuwD/UwIocvLy3Q6bVmWoiiEkGw2OzMzI8sywzAvLy+5XK5j86tQSZKy2SyEUNO04+NjAMD09PTm5mY4HDYMo1QqnZycvL6+/gGUoqidnR2EkOM4e3t7Dw8Pw8PDsiz7vq8oyt3d3fX19f39ved5ndv7DRRCODExAQCoVqvFYvHo6EgQBFEUHx8fLy4uFEXRNK3VahFCuufRBwoh7JxgGGZycrJQKGiaVqvVRFEsl8uqqt7e3lYqFcuyHMcBACCEuqt6oRRFBfsMABgYGBAEwTTN8/Pzubm5RCJB07RhGLqu97jrjhMAAPV4TKfToiiOjIwwDDM6OhqJRDDGmqaZpimKYjCZZrMZ5NJ/r/7tY4xnZ2eXl5dN07y6ugqFQp7n1Wo1lmVXVlZkWX56etrd3f24zj1f3kGHhobW19fn5+dLpRJCqFgsWpYVjUYXFhaWlpY4jguHw57nfRxDj362DyHkeZ7juFarFYvFCCHtdrvRaGQymY2NDY7jIISnp6e/SF0fpxRFiaIYj8d937dtu1wuQwglSVpcXEylUgCAZrO5v7//W+I7p4QQwzBarRbLsrZtp1KpeDzO8zzP841Go1qtrq2tdcfmS05936/Vajc3N5IktdttVVXr9bpt24SQfD5/dnaWz+f79v7xwXz3SNM0nUwmY7EYRVGmaVqWhTGORCL1er1SqXQHsZsYrP2n0OBVD+JBCCGEBDWu635l6P2hQSMdaM///0mBu+9hdUO/mfgt+gE1HbgOQ9UbbwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133A5F0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAC7UlEQVR4nO2VvUvrUBjGc3ISTgxUbBRKGxSDiHQRoQ4FXYrOiqDgnMF/w82ODnZoByeHgpNQ0KWggosguFVBjbiJFqJJbT7O1x0OFK/fvXrvdJ8x5P2d533eNyeS9BcEui4AAAAgSRJj7L135K6IEMKFhYWtra2lpSVN07o19IZUVd3Y2PB9P45jx3EKhYKw/OdSFGV3d5cQwhijlF5dXY2MjMhyd42+lG3bGGNBbDQa+XweQvgt4uDgYBAEnHPGmOM4mUzmBxq/ubnhnHPOHx4eksnkt3BCpVKJMcY5j6JoaGjoB4i5XC6OY845pdS27e92LUmSYRj39/eMMUJIuVxWVVU8hxAmEon+/n5d17tbAAjh9vZ2GIZhGJbLZV3XIYQIobm5uXq9fnd357ru4eGhZVlftQ8AmJ+fd13X87z19fXe3l6EUDKZrFQqcRyLiBljURRVKhWE0Je4AwMDjuP4vr+3t2eaJkLIMIydnR2MsVgDxhhjLAiCer2eyWRkWX7OVV4TFUVZW1szTZNS2mw2JUlKJBK2bc/MzEAIxdDa7bbruufn55ubm5xzAMDz++UNqGEYhUJB1KfT6Ww2m0qlVlZWFEWhlAZBcHR0VK1WG41Gs9lstVq+73POf7P1Ok1d1+M4xhhjjGVZnpycHB0dFYPCGB8cHKyurl5fXxNCMMaUUhHFR1DO+dPT0/7+PsY4CIKTk5Pb29upqam+vj4IYRRFvu9HUSQOEOG+sPl2+61Wq1qtnp2dPT4+cs6Hh4cnJiY0TQMAaJo2Pj4+PT1dq9XCMOz4+BxKCLm8vPQ8L5VKzc7OLi8vp9PpznDFWFRVRQhhjEVin2TaqVRVdWxsbHFx0TRNccsxxi4uLorF4vHxsed5lFLBen7eu1DOOULIsqx8Pt/T00MplSSp3W4Xi8VSqRSGIWMMACDWQxDF8nYIb3wJAACEUC6Xsywrm80SQmq12unpqaA/f+2FlY+gnZpO2Qc/zv/6R/oFSWqyBgcLF4EAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133BD00>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADI0lEQVR4nO2Uv0vrUBTH74+kSUxsVFAUFwc1lIJIqdJJRdCC+Ac4ODk4uLi6u4mLf4CZnQsOXVUEwUGoxQ61goNW05qamKa5yb15Q6Dv8V5t3/NND94Zwk3C+XDO+Z7vBeB//BMB/yoZQoQQhJAxxhhrf+e+AIoOCCGe50VRFEWxXq+HYRiG4R9AIYQ8z6uqynGc7/uEEEqpqqrT09MYY8MwHMf5+Pj43UoRQoIgzMzM7OzsaJr28vJyfn5+dXWFEFpeXk4kErZt67rerrEHFEIoy/Lq6uru7m46nY7FYoyxer1OKR0dHc1msxMTEwCAYrHY39/ved6n0Pa8BEFIpVIHBwezs7OSJEVSuK779PSEMV5ZWdE0DWNMCLEsq1AoUEp/KvY7ESGEMY7H41tbW+Vy2fd9SimllBBSLpf39vY2NzePj4/f398juV9fX7PZLMb402YjNRRFWVtbKxQKhBDGWBAEjUZD1/VUKjU2Nra4uHh9fe37PmOMEKLrejwe76DEjy9hGIqimEwmR0ZGMMZhGLZarbOzM13X397eZFnOZDKTk5MIIcZYpVI5OjpyHOdXKNfGRQeMsaqqAIBoTI7jWJaVSCSCIAAALC0tSZIEALBte39/v1Qq/bjznYWKKPl8fnh4eH5+nuO4Wq1mGEaj0RBFcX19fW5uDiHkeV4ul8vlcoSQjvr8bFOEEEJIFMW+vj5FUQYHB2VZxhhnMpnt7e3x8XEAwN3d3cbGRqlU6qx494ikGxgYWFhYyOfzzWbT933btg8PD0VR7JKIuvwLwzAIAkJIOp1OJpMAgFarValUTk5OCCFdEnvYNGpwamqKUmrb9uPj4+npabVa7Z7V+0IRBMG27fv7e8MwLi8vLy4uXNdte+8rUAhhLBZzHOf29vbm5qZYLJqmGTnlr6CSJJmmaVlWtVo1DMM0Tc/zMMaf+r3nzQ8hVBRF0zSe55+fn2u1mu/7PM9jjJvNZhAEHbm9Z+q67sPDA8dxhJBo2yGEQ0NDjDHHcb4IRQhRSoMg8DwvMmX09H3/s/Z7QzmOiy7T9hAppdEcvuKo7tFlAb4B6rTGdW80Gz8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133B850>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAADVUlEQVR4nO2VPUjzXBTHc5OmSao2tX40NYoWVFoVp1LRSagggpOLi4Pg0MmCk4uDCE4iOAR0ET8GRUTRwYIgSEWrg0Mp0i5tVGpbbKP9DGlMYp7B7SHV+vIuL7y//fzO/957DheC/uffBvy6AACdToeiKEEQGIbV1taSJCkIQjqdLhQKoiiqqqr7lc5oNFqtVhzHCYIgSXJgYKCnp4eiKEEQ/H7/xsZGuVyGIAiu0ojj+M7OTiAQmJ+fJ0myWCzyPE8QRGNjo8ViMZlMEAQ1NTXBMAxBUFVJcRyPxWIWi0UQhKOjo4eHh1KpVFdXp6qq2WyWZTkcDt/c3CQSCVVVq5IiCBIMBq1Wq6IoBwcHKysrPM9DECRJkizLLMuGw2G/3x+NRgVBqFY6Ozvb1dX1+fl5cXHh9XoFQQAAmEymsbExu90uSdLJyQnLsoqifBl/hiTJXC6nKEosFqMoCoZhBEHa2tqOj4+fn599Pp/T6UQQpCrXFwCAubm5crmczWY9Ho/BYKipqRkcHAyFQoIgZDIZj8ej02mc9bvjEwTR2toaDAavr6/v7+/b29tHR0cXFhaMRqMoiru7u1tbW7Isa6SpZEQQpKOjw2azcRyn1+ttNtvMzIzL5dLr9aIorq+vLy4ufnx8aNZWTEpRlMPhSKVSmUxmaGhoenq6u7s7n88XCgWGYba3tysZKyY1Go2rq6sOh+Pp6Umv19M03dLSEo/HI5FIKBTa29vLZrOVjNpJAQBer3dychLDMJfLJctyLpe7vb2NRCLv7++vr69fc/oNGmtqNpunpqYMBgOKojAM5/N5hmHW1tZ4nnc6nYqiKIryvVQDp9OZTqclSRJF8e7urrOzE8dxmqYvLy8DgUBvb++Phr+TAgAoihJFkeO4/f39iYkJlmUlServ77fb7YVCIRqN/ijVuFNJkuLxOMdxm5ubHMd9tVleXsZx/PT09JtHrygFABSLxWQyyXEciqIWi6W+vp5hGIfDwfO8z+erZsH/lqqqmkgkSqVSc3Oz2+0eGRlxu919fX2KohweHr68vPxo1MZgMIyPj5+fn19dXT0+Pr69vSWTyaWlJQzDqjRoD79Op2toaKBpenh4mOf5s7OzVCr1TyZJuycAAPz6c/zv8AcGgpH0UXzgTgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F23F133B280>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACtUlEQVR4nO2UwUrrQBSGMzOZmNSGEolFRRBEkYAu1I1uXLlwJ26EvkB3Po0r9RlcuXRRUCpqQVEQRXQhWqw1sTFOk5DJzLgIV7y12MbLXVy4/26GM9+cc+Y/I0n/9U8I/OwYhBAhhBBSVRUh5HleHMdCiNRQhNDAwMDMzMzs7KxlWZRSCKGqqoSQ7e3t3d1d3/eTSLkbHACgUChsbm6qqipJUhzHURQRQnzfv7m5oZTm83kI4Ud8ZyhCaGdnZ2lpCQAghAiCoNFovL6+uq5brVZt2z49PS2VSkEQdAUFAGiatr6+/kE8OTlZW1vL5/OLi4tDQ0OHh4eXl5cXFxf1ep1z3lXJGOOVlZXn52fGGKX06OhofHy8r69veXm5Uqns7e3Nzc1lMhmEEAC/vc13mWqahjF2HOft7e329rZUKo2MjKyurhaLxUwms7W1dXZ29rnqrjKFEBqGMTk5OT8/v7GxcXV11Wg0wjAMw7BcLuu6ngLXgsYYT0xM3N/fc84551EUnZ+fG4aRjvJ1x7Isz/OEEIyxg4ODxFh/BJUkyTAMx3EYY09PT91UDVvWH6P2WRhj27bjOL6+viaEpIZ+FQCgp6cnGUrXddvemg6aeEDXdVmWJUkKgqBtf1rU6tNkhIUQCU6WZQBAf39/GIaMsVqt1pHYBpqwFEXRNC2XyzHGCCHNZrNer+dyuXK5DCFkjKWDJjmqqjo4ODg8POy6ruM42Ww2CIJKpfLw8PCT8hMzhmHo+/7d3V0URRjjqakp27YppYQQRVEYY98/VxuoEAJjnM1mFUUxTdM0zdHRUdM09/f3Pc+jlCY/VgpoIoRQHMeUUsuyFhYWxsbGarXa4+Pjy8tL8sV9z23TIEVRdF3v7e1tNpuyLE9PTxuGwTk/Pj6uVquU0o5W7dx18Euc826c/7f0DglYZ9wg0e4MAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The recreated MNIST images are still blurry due to the slow converge of the overall loss.This shows that the hyperparameters can further be fine-tuned like learning rate,batch_size,number of epochs,etc.. to make the images look more sharp"
      ],
      "metadata": {
        "id": "FF5rtDN4XSUr"
      }
    }
  ]
}